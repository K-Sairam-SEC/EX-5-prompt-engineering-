# Ex. No. 5 – Comparative Analysis of Naïve Prompting versus Basic Prompting Using ChatGPT Across Various Test Scenarios

### Date: 21/10/2025
### Register No.: 25000527

# Aim:

To test how ChatGPT responds to naïve prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios, and to analyze the quality, accuracy, and depth of the generated responses.

# Algorithm:

Define Prompt Types:

Naïve Prompts: Vague, open-ended, or lacking context.

Basic Prompts: Clear, structured, and detailed with explicit instructions.

Prepare Test Scenarios:
Five different scenarios were selected to evaluate prompt effectiveness:

Creative storytelling

Factual question answering

Concept summarization

Advice or recommendation

Technical explanation

Create Prompt Pairs:
For each scenario, one naïve and one basic prompt were created to perform the same task.

Run Experiments:

The naïve prompt was input into ChatGPT, and its response was recorded.

The basic prompt was then input for comparison.

Evaluate Responses:

Each response was assessed on quality, accuracy, and depth.

Observations were made on how the prompt’s clarity affected output performance.

Analyze Results:

Compare outputs from both prompt types.

Determine whether refined prompts produce better, more useful results.

Procedure:

Opened ChatGPT and prepared a test environment for consistent evaluation.

Entered a set of naïve prompts representing minimal input guidance.

Entered corresponding basic prompts with more context, instructions, or structure.

Recorded both responses in a comparison table.

Evaluated each pair according to three metrics — quality, accuracy, and depth.

Compiled results into a summarized analysis table.


| Prompt Type        | Example Prompt                                                                                         |
|--------------------|--------------------------------------------------------------------------------------------------------|
| Simple             | "A person walking in a park."                                                                          |
| Descriptive        | "A person in a red jacket walking in a sunny park with birds flying and a dog running beside them."   |
| Time/Motion-Based  | "Time-lapse of sunset over the ocean with camera zooming out slowly."                                  |
| Stylized/Artistic  | "Animated scene of a futuristic city at night with neon lights and flying cars."                       |
| Cinematic          | "A car chase in a neon-lit city with reflections on wet roads and camera panning."                    |


Across all five scenarios, basic prompts produced significantly better responses in clarity, relevance, and depth.

Naïve prompts resulted in vague or repetitive answers lacking direction.

Providing explicit details or constraints improved factual precision and creativity.

The model performed best when given structured guidance (e.g., tone, length, or target audience).

This demonstrates that prompt engineering directly influences output quality.

Conclusion:

Basic prompting consistently outperforms naïve prompting in terms of response quality, factual accuracy, and contextual depth.

Well-crafted prompts enable ChatGPT to interpret intent more effectively and produce more meaningful results.

No test case showed naïve prompting performing equal to or better than basic prompting.

Therefore, clear and structured prompt design is essential for maximizing the potential of conversational AI models.

Result:

The prompt for the above-said problem executed successfully, and the comparative analysis confirmed that refined prompting yields superior AI responses across diverse test scenarios.
