# Ex. No. 5 – Comparative Analysis of Naïve Prompting versus Basic Prompting Using ChatGPT Across Various Test Scenarios

### Date: 21/10/2025
### Register No.: 25000527

# Aim:

To test how ChatGPT responds to naïve prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios, and to analyze the quality, accuracy, and depth of the generated responses.

# Algorithm:

Define Prompt Types:

Naïve Prompts: Vague, open-ended, or lacking context.

Basic Prompts: Clear, structured, and detailed with explicit instructions.

Prepare Test Scenarios:
Five different scenarios were selected to evaluate prompt effectiveness:

Creative storytelling

Factual question answering

Concept summarization

Advice or recommendation

Technical explanation

Create Prompt Pairs:
For each scenario, one naïve and one basic prompt were created to perform the same task.

Run Experiments:

The naïve prompt was input into ChatGPT, and its response was recorded.

The basic prompt was then input for comparison.

Evaluate Responses:

Each response was assessed on quality, accuracy, and depth.

Observations were made on how the prompt’s clarity affected output performance.

Analyze Results:

Compare outputs from both prompt types.

Determine whether refined prompts produce better, more useful results.

Procedure:

Opened ChatGPT and prepared a test environment for consistent evaluation.

Entered a set of naïve prompts representing minimal input guidance.

Entered corresponding basic prompts with more context, instructions, or structure.

Recorded both responses in a comparison table.

Evaluated each pair according to three metrics — quality, accuracy, and depth.

Compiled results into a summarized analysis.

Results (Text Format):

1. Creative Story

Naïve Prompt: “Tell me a story.”
Response: Produced a short, generic story about a boy and his dog with minimal plot or detail.

Basic Prompt: “Write a 300-word fantasy story about a brave girl fighting a dragon.”
Response: Generated a vivid and structured fantasy story with emotional depth, clear progression, and strong descriptive language.
→ Result: The basic prompt led to a much richer and more coherent narrative.

2. Factual Question Answering

Naïve Prompt: “Tell me about Mars.”
Response: Provided general surface-level facts with limited depth.

Basic Prompt: “List five key facts about Mars’ atmosphere and surface conditions.”
Response: Delivered organized, accurate, and specific information about the planet’s composition and environment.
→ Result: The basic prompt gave more precise and factual content.

3. Concept Summarization

Naïve Prompt: “Summarize photosynthesis.”
Response: Produced an oversimplified explanation lacking structure.

Basic Prompt: “Summarize the process of photosynthesis in 3–4 lines for students.”
Response: Clear and concise explanation describing sunlight absorption, chlorophyll function, and glucose formation.
→ Result: The basic prompt yielded a more accurate and educational summary.

4. Advice or Recommendation

Naïve Prompt: “Give me some advice.”
Response: Generic motivational advice with no specific context.

Basic Prompt: “Give 3 productivity tips for college students managing their time effectively.”
Response: Offered practical, actionable, and student-focused tips.
→ Result: The basic prompt was more useful and targeted to the user’s context.

5. Technical Explanation

Naïve Prompt: “Explain AI.”
Response: High-level definition with limited clarity and no examples.

Basic Prompt: “Explain artificial intelligence in simple terms for a 12-year-old, with real-life examples.”
Response: Provided an easy-to-understand explanation with relatable examples like virtual assistants and self-driving cars.
→ Result: The basic prompt gave a clearer and more engaging response suitable for the target audience.

Overall Observation:

Across all test scenarios, basic prompts consistently produced responses that were:

Higher in quality (better language, structure, and focus)

More accurate (factually precise and contextually correct)

Deeper in content (richer explanation and creativity)



Across all five scenarios, basic prompts produced significantly better responses in clarity, relevance, and depth.

Naïve prompts resulted in vague or repetitive answers lacking direction.

Providing explicit details or constraints improved factual precision and creativity.

The model performed best when given structured guidance (e.g., tone, length, or target audience).

This demonstrates that prompt engineering directly influences output quality.

Conclusion:

Basic prompting consistently outperforms naïve prompting in terms of response quality, factual accuracy, and contextual depth.

Well-crafted prompts enable ChatGPT to interpret intent more effectively and produce more meaningful results.

No test case showed naïve prompting performing equal to or better than basic prompting.

Therefore, clear and structured prompt design is essential for maximizing the potential of conversational AI models.

Result:

The prompt for the above-said problem executed successfully, and the comparative analysis confirmed that refined prompting yields superior AI responses across diverse test scenarios.
